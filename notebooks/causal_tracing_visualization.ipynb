{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Causal Tracing Visualization\n",
    "\n",
    "This notebook demonstrates causal tracing on Mamba State Space Models and visualizes the results as heatmaps (similar to the ROME paper).\n",
    "\n",
    "The heatmap shows which (layer, position) pairs are critical for factual recall:\n",
    "- **X-axis**: Token positions in the prompt\n",
    "- **Y-axis**: Model layers (0 = earliest, higher = later)\n",
    "- **Color**: Probability of correct answer when that state is restored\n",
    "  - Blue/Dark: Low probability (restoration doesn't help)\n",
    "  - Yellow/Bright: High probability (restoration recovers the answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from mamba_causal_analysis.mamba_models import load_mamba_model\n",
    "from mamba_causal_analysis.mamba_causal_trace import calculate_hidden_flow\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Mamba Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mamba-130m (smallest model for quick experimentation)\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "mt = load_mamba_model(model_name, device=device)\n",
    "print(f\"✓ Model loaded with {mt.num_layers} layers\")\n",
    "print(f\"  Device: {mt.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Test Prompt\n",
    "\n",
    "Choose a factual prompt where we know the answer. We'll trace which parts of the model are responsible for recalling this fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example factual prompts (choose one or add your own)\n",
    "test_cases = [\n",
    "    {\"prompt\": \"The Eiffel Tower is located in\", \"subject\": \"Eiffel Tower\"},\n",
    "    {\"prompt\": \"The Space Needle is located in downtown\", \"subject\": \"Space Needle\"},\n",
    "    {\"prompt\": \"The mother tongue of Angela Merkel is\", \"subject\": \"Angela Merkel\"},\n",
    "    {\"prompt\": \"Apple Inc. was founded by Steve\", \"subject\": \"Apple Inc\"},\n",
    "]\n",
    "\n",
    "# Select test case\n",
    "test = test_cases[0]  # Change index to try different prompts\n",
    "prompt = test[\"prompt\"]\n",
    "subject = test[\"subject\"]\n",
    "\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Subject: \\\"{subject}\\\"\")\n",
    "print()\n",
    "\n",
    "# Test what the model predicts\n",
    "tokens = mt.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = mt.model(tokens[\"input_ids\"])\n",
    "    if hasattr(outputs, 'logits'):\n",
    "        logits = outputs.logits\n",
    "    else:\n",
    "        logits = outputs\n",
    "    \n",
    "    probs = torch.softmax(logits[0, -1, :], dim=0)\n",
    "    top_tokens = torch.topk(probs, 5)\n",
    "    \n",
    "print(\"Model's top 5 predictions:\")\n",
    "for prob, token_id in zip(top_tokens.values, top_tokens.indices):\n",
    "    token_str = mt.tokenizer.decode([token_id.item()])\n",
    "    print(f\"  {token_str:20s} {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Causal Tracing\n",
    "\n",
    "This cell will:\n",
    "1. Run the clean input and save all hidden states\n",
    "2. For each (layer, position) pair:\n",
    "   - Corrupt the input with noise\n",
    "   - Restore the clean state at that location\n",
    "   - Measure how much this helps recover the correct answer\n",
    "3. Return a matrix of scores for visualization\n",
    "\n",
    "**Note**: This takes a few minutes (tracing all layer × position combinations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run causal tracing\n",
    "samples = 10  # Number of noise samples to average (10 is fast, 100 is more accurate)\n",
    "noise_level = 3.0  # Standard deviations of noise (3.0 is standard from ROME)\n",
    "\n",
    "print(f\"Running causal tracing with {samples} noise samples...\")\n",
    "print(f\"Tracing {mt.num_layers} layers × sequence length positions\")\n",
    "print(f\"Estimated time: ~{mt.num_layers * samples * 0.1:.1f} seconds\")\n",
    "print()\n",
    "\n",
    "result = calculate_hidden_flow(\n",
    "    mt,\n",
    "    mt.tokenizer,\n",
    "    prompt=prompt,\n",
    "    subject=subject,\n",
    "    samples=samples,\n",
    "    noise_level=noise_level,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Causal tracing complete!\")\n",
    "print(f\"  Clean probability: {result['high_score']:.4f}\")\n",
    "print(f\"  Corrupted probability: {result['low_score']:.4f}\")\n",
    "print(f\"  Effect size: {result['high_score'] - result['low_score']:.4f}\")\n",
    "print(f\"  Target token: {result['target_token']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Heatmap\n",
    "\n",
    "The heatmap shows:\n",
    "- **Bright (yellow) regions**: Restoring states here recovers the answer\n",
    "- **Dark (blue) regions**: Restoring states here doesn't help\n",
    "\n",
    "This reveals which layers and token positions encode the factual knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_causal_trace_heatmap(result, title=None):\n",
    "    \"\"\"\n",
    "    Plot causal tracing heatmap like in the ROME paper.\n",
    "    \"\"\"\n",
    "    scores = result['scores']\n",
    "    tokens = result['input_tokens']\n",
    "    low_score = result['low_score']\n",
    "    high_score = result['high_score']\n",
    "    subj_start, subj_end = result['subject_range']\n",
    "    \n",
    "    # Normalize scores to [0, 1] range\n",
    "    normalized_scores = (scores - low_score) / (high_score - low_score)\n",
    "    normalized_scores = np.clip(normalized_scores, 0, 1)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(\n",
    "        normalized_scores,\n",
    "        aspect='auto',\n",
    "        cmap='RdYlBu_r',  # Red-Yellow-Blue reversed (blue=low, red=high)\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        interpolation='nearest'\n",
    "    )\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Normalized Probability', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Axes labels\n",
    "    ax.set_xlabel('Token Position', fontsize=12)\n",
    "    ax.set_ylabel('Layer', fontsize=12)\n",
    "    \n",
    "    # Set token labels on x-axis\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    \n",
    "    # Highlight subject tokens\n",
    "    ax.axvline(subj_start - 0.5, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axvline(subj_end + 0.5, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # Add text box with subject\n",
    "    ax.text(\n",
    "        (subj_start + subj_end) / 2,\n",
    "        -1,\n",
    "        'Subject',\n",
    "        ha='center',\n",
    "        va='top',\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "    \n",
    "    # Title\n",
    "    if title is None:\n",
    "        title = f\"Causal Tracing: {result.get('prompt', prompt)}\"\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    \n",
    "    # Grid\n",
    "    ax.set_yticks(range(0, scores.shape[0], 4))\n",
    "    ax.grid(True, alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot the heatmap\n",
    "fig = plot_causal_trace_heatmap(result)\n",
    "plt.show()\n",
    "\n",
    "# Find and print most important positions\n",
    "scores = result['scores']\n",
    "max_layer, max_pos = np.unravel_index(scores.argmax(), scores.shape)\n",
    "print(f\"\\nMost important restoration:\")\n",
    "print(f\"  Layer {max_layer}, Position {max_pos}\")\n",
    "print(f\"  Token: '{result['input_tokens'][max_pos]}'\")\n",
    "print(f\"  Score: {scores[max_layer, max_pos]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results\n",
    "\n",
    "Let's examine which layers are most important for factual recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average effect by layer\n",
    "scores = result['scores']\n",
    "layer_avg = scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layer_avg, marker='o')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average Restoration Effect')\n",
    "plt.title('Average Effect of Restoring Each Layer')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 most important layers (by average effect):\")\n",
    "top_layers = np.argsort(layer_avg)[-5:][::-1]\n",
    "for layer in top_layers:\n",
    "    print(f\"  Layer {layer:2d}: {layer_avg[layer]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subject Token Analysis\n",
    "\n",
    "How important are the subject tokens specifically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_start, subj_end = result['subject_range']\n",
    "subject_scores = scores[:, subj_start:subj_end+1]\n",
    "\n",
    "print(f\"Subject tokens: {result['input_tokens'][subj_start:subj_end+1]}\")\n",
    "print(f\"Subject token range: positions {subj_start} to {subj_end}\")\n",
    "print()\n",
    "\n",
    "# Average effect across subject positions by layer\n",
    "subject_layer_avg = subject_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(subject_layer_avg, marker='o', label='Subject tokens')\n",
    "plt.plot(layer_avg, marker='s', alpha=0.5, label='All tokens')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average Restoration Effect')\n",
    "plt.title('Effect of Restoring Subject vs All Tokens')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Overall average effect (all tokens): {layer_avg.mean():.4f}\")\n",
    "print(f\"Average effect (subject tokens): {subject_layer_avg.mean():.4f}\")\n",
    "print(f\"Ratio (subject / all): {subject_layer_avg.mean() / layer_avg.mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results (Optional)\n",
    "\n",
    "Save the causal tracing results for later analysis or comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save\n",
    "# output_path = f\"../results/{prompt.replace(' ', '_')[:30]}_trace.npz\"\n",
    "# Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "# np.savez(\n",
    "#     output_path,\n",
    "#     scores=result['scores'],\n",
    "#     low_score=result['low_score'],\n",
    "#     high_score=result['high_score'],\n",
    "#     input_tokens=result['input_tokens'],\n",
    "#     subject_range=result['subject_range'],\n",
    "#     target_token=result['target_token'],\n",
    "#     prompt=prompt,\n",
    "#     subject=subject,\n",
    "# )\n",
    "# print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Try different prompts and analyze the patterns:\n",
    "- Do certain layers consistently encode factual knowledge?\n",
    "- How does the pattern differ for different types of facts?\n",
    "- Compare with GPT-2 results from the ROME paper\n",
    "\n",
    "For Phase 3, we'll dive deeper into tracing the internal SSM state (h_t) and selection parameters (B, C, Δt)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
